{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mDjg9pgTw6ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDrarYhSxFdJ",
        "outputId": "7f1fc810-eff7-4116-bdcb-bbb50f574a58"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba) (67.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuGmHYhRw5Zv",
        "outputId": "c82243e6-1c40-4597-a19b-5bc4d2bfa3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: [ 8. 10.  7.  9.  1.]\n",
            "B: [5. 9. 8. 6. 8.]\n",
            "Dot Product: 248.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "\n",
        "@cuda.jit\n",
        "def dotProd(d_a, d_b, d_dotprod, N):\n",
        "    sdata = cuda.shared.array(32, dtype=float32)  # Shared memory for parallel reduction\n",
        "\n",
        "    tx = cuda.threadIdx.x\n",
        "    bx = cuda.blockIdx.x\n",
        "    bw = cuda.blockDim.x\n",
        "\n",
        "    start = tx\n",
        "    stride = bw\n",
        "\n",
        "    dot_product = 0.0\n",
        "\n",
        "    # Compute element-wise products and perform parallel reduction in shared memory\n",
        "    while start < N:\n",
        "        dot_product += d_a[start] * d_b[start]\n",
        "        start += stride\n",
        "\n",
        "    sdata[tx] = dot_product\n",
        "\n",
        "    # Synchronize !!\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # parallel reduction in shared memory\n",
        "    i = cuda.blockDim.x // 2\n",
        "    while i != 0:\n",
        "        if tx < i:\n",
        "            sdata[tx] += sdata[tx + i]\n",
        "        cuda.syncthreads()\n",
        "        i //= 2\n",
        "\n",
        "    # Store the final result in global memory\n",
        "    if tx == 0:\n",
        "        d_dotprod[bx] = sdata[0]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    N = 5\n",
        "\n",
        "    # Allocate host memory\n",
        "    h_a = np.random.randint(1, 11, N).astype(np.float32)\n",
        "    h_b = np.random.randint(1, 11, N).astype(np.float32)\n",
        "    h_dotprod = np.zeros(1, dtype=np.float32)\n",
        "\n",
        "    # Allocate device memory\n",
        "    d_a = cuda.to_device(h_a)\n",
        "    d_b = cuda.to_device(h_b)\n",
        "    d_dotprod = cuda.device_array(1, dtype=np.float32)\n",
        "\n",
        "    # Launch kernel\n",
        "    block_size = 32\n",
        "    grid_size = (N + block_size - 1) // block_size\n",
        "    dotProd[grid_size, block_size](d_a, d_b, d_dotprod, N)\n",
        "\n",
        "    # Copy result to host\n",
        "    d_dotprod.copy_to_host(h_dotprod)\n",
        "\n",
        "    # Print result\n",
        "    print(\"A:\", h_a)\n",
        "    print(\"B:\", h_b)\n",
        "    print(\"Dot Product:\", h_dotprod[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "import math\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def block_reduce(a):\n",
        "    # Perform parallel reduction within a block\n",
        "    shared_val = cuda.shared.array(32, dtype=float32)\n",
        "    tid = cuda.threadIdx.x\n",
        "    bid = cuda.blockIdx.x\n",
        "    bdim = cuda.blockDim.x\n",
        "\n",
        "    shared_val[tid] = a\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # Perform reduction in shared memory\n",
        "    for s in range(bdim // 2):\n",
        "        if tid < bdim // 2:\n",
        "            shared_val[tid] += shared_val[tid + s + 1]\n",
        "        cuda.syncthreads()\n",
        "\n",
        "    return shared_val[0]\n",
        "\n",
        "@cuda.jit\n",
        "def mean_var_norm(d_a, d_mean, d_var, d_norm, N):\n",
        "    col = cuda.threadIdx.x\n",
        "    stride = cuda.blockDim.x\n",
        "\n",
        "    # Compute mean using parallel reduction(PR)\n",
        "    mean_val = 0.0\n",
        "    for i in range(col, N, stride):\n",
        "        mean_val += d_a[i]\n",
        "\n",
        "    mean_val = cuda.atomic.add(d_mean, 0, mean_val)\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # Compute variance using PR\n",
        "    var_val = 0.0\n",
        "    for i in range(col, N, stride):\n",
        "        var_val += (d_a[i] - d_mean[0]) ** 2\n",
        "\n",
        "    var_val = cuda.atomic.add(d_var, 0, var_val)\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # Synchronize to make sure mean and variance are calculated\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # Compute normalized values\n",
        "    for i in range(col, N, stride):\n",
        "        d_norm[i] = (d_a[i] - d_mean[0]) / (math.sqrt(d_var[0] / N + 1e-8))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    N = 5\n",
        "\n",
        "    # Allocate host memory\n",
        "    h_a = np.random.randint(1, 11, N).astype(np.float32)\n",
        "    h_mean = np.zeros(1, dtype=np.float32)\n",
        "    h_var = np.zeros(1, dtype=np.float32)\n",
        "    h_norm = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "    # Allocate device memory\n",
        "    d_a = cuda.to_device(h_a)\n",
        "    d_mean = cuda.device_array(1, dtype=np.float32)\n",
        "    d_var = cuda.device_array(1, dtype=np.float32)\n",
        "    d_norm = cuda.device_array(N, dtype=np.float32)\n",
        "\n",
        "    # Launch kernel\n",
        "    block_size = 32\n",
        "    grid_size = (N + block_size - 1) // block_size\n",
        "    mean_var_norm[grid_size, block_size](d_a, d_mean, d_var, d_norm, N)\n",
        "\n",
        "    # Copy results to host\n",
        "    d_mean.copy_to_host(h_mean)\n",
        "    d_var.copy_to_host(h_var)\n",
        "    d_norm.copy_to_host(h_norm)\n",
        "\n",
        "\n",
        "    print(\"A:\", h_a)\n",
        "    print(\"Mean:\", h_mean[0])\n",
        "    print(\"Variance:\", h_var[0])\n",
        "    print(\"Normalized Values:\", h_norm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw0AEK0TxL8U",
        "outputId": "e7e1683b-8bbc-41e7-831b-08cc130fdaea"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: [8. 3. 1. 6. 2.]\n",
            "Mean: 20.0\n",
            "Variance: 1314.0\n",
            "Normalized Values: [-0.7402332  -1.0486637  -1.1720359  -0.86360544 -1.1103498 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "\n",
        "# Matrix size\n",
        "M, N = 512, 512\n",
        "\n",
        "@cuda.jit\n",
        "def matAdd(d_A, d_B, d_C):\n",
        "    # Allocate shared memory for matrices A, B, and C\n",
        "    s_A = cuda.shared.array(shape=(16, 16), dtype=float32)\n",
        "    s_B = cuda.shared.array(shape=(16, 16), dtype=float32)\n",
        "    s_C = cuda.shared.array(shape=(16, 16), dtype=float32)\n",
        "\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "    bx = cuda.blockIdx.x\n",
        "    by = cuda.blockIdx.y\n",
        "\n",
        "    # Load elements into shared memory\n",
        "    row_A = by * 16 + ty\n",
        "    col_A = bx * 16 + tx\n",
        "    row_B = by * 16 + ty\n",
        "    col_B = bx * 16 + tx\n",
        "\n",
        "    s_A[ty, tx] = d_A[row_A, col_A]\n",
        "    s_B[ty, tx] = d_B[row_B, col_B]\n",
        "\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # Perform matrix addition in shared memory\n",
        "    s_C[ty, tx] = s_A[ty, tx] + s_B[ty, tx]\n",
        "\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # Store the result in global memory\n",
        "    row_C = by * 16 + ty\n",
        "    col_C = bx * 16 + tx\n",
        "    d_C[row_C, col_C] = s_C[ty, tx]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    h_A = np.random.rand(M, N).astype(np.float32)\n",
        "    h_B = np.random.rand(M, N).astype(np.float32)\n",
        "    h_C = np.empty_like(h_A)\n",
        "\n",
        "    # Allocate device memory\n",
        "    d_A = cuda.to_device(h_A)\n",
        "    d_B = cuda.to_device(h_B)\n",
        "    d_C = cuda.device_array_like(h_C)\n",
        "\n",
        "    # Define block and grid dimensions\n",
        "    block_dim = (16, 16)\n",
        "    grid_dim = ((N + block_dim[1] - 1) // block_dim[1], (M + block_dim[0] - 1) // block_dim[0])\n",
        "\n",
        "    # Launch kernel\n",
        "    matAdd[grid_dim, block_dim](d_A, d_B, d_C)\n",
        "\n",
        "    # Copy result to host\n",
        "    d_C.copy_to_host(h_C)\n",
        "\n",
        "    # Display results\n",
        "    print(\"Matrix A:\")\n",
        "    print(h_A)\n",
        "    print(\"----------\")\n",
        "    print(\"Matrix B:\")\n",
        "    print(h_B)\n",
        "    print(\"----------\")\n",
        "    print(\"Matrix C (Result of Addition):\")\n",
        "    print(h_C)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64moHlPNy52w",
        "outputId": "8814a0c6-c416-437c-fde0-e8f1b39d2c66"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "[[0.633758   0.33405107 0.5288251  ... 0.5341994  0.94313174 0.87808603]\n",
            " [0.99936557 0.4754105  0.7203953  ... 0.8777755  0.8494717  0.6279307 ]\n",
            " [0.1533908  0.778205   0.44190192 ... 0.9093095  0.35668865 0.81189936]\n",
            " ...\n",
            " [0.6389082  0.73845655 0.6936545  ... 0.09694602 0.09927858 0.68780136]\n",
            " [0.5997956  0.8147846  0.06913508 ... 0.8301413  0.9555761  0.49095625]\n",
            " [0.5183593  0.7990846  0.5602129  ... 0.8253773  0.3116791  0.6956825 ]]\n",
            "----------\n",
            "Matrix B:\n",
            "[[0.82938707 0.7934714  0.8981924  ... 0.41581234 0.9609986  0.15894851]\n",
            " [0.35991558 0.9129847  0.74222845 ... 0.25218287 0.5928586  0.50405324]\n",
            " [0.7566857  0.8522018  0.476235   ... 0.12081056 0.49309912 0.98133796]\n",
            " ...\n",
            " [0.30231717 0.8695147  0.33973026 ... 0.36438474 0.39720768 0.34914818]\n",
            " [0.06269992 0.5950702  0.64461607 ... 0.793343   0.9580506  0.32525393]\n",
            " [0.30928665 0.14187622 0.05999177 ... 0.82270384 0.97562814 0.73048717]]\n",
            "----------\n",
            "Matrix C (Result of Addition):\n",
            "[[1.463145   1.1275225  1.4270175  ... 0.95001173 1.9041303  1.0370345 ]\n",
            " [1.3592812  1.3883952  1.4626238  ... 1.1299584  1.4423304  1.131984  ]\n",
            " [0.9100765  1.6304069  0.91813695 ... 1.03012    0.8497878  1.7932373 ]\n",
            " ...\n",
            " [0.9412254  1.6079712  1.0333848  ... 0.46133077 0.49648625 1.0369495 ]\n",
            " [0.6624955  1.4098548  0.71375114 ... 1.6234844  1.9136267  0.81621015]\n",
            " [0.82764596 0.9409608  0.6202047  ... 1.6480811  1.2873073  1.4261696 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "\n",
        "@cuda.jit\n",
        "def matMul(d_A, d_B, d_C):\n",
        "    row, col = cuda.grid(2)\n",
        "\n",
        "    if row < d_C.shape[0] and col < d_C.shape[1]:\n",
        "        M, N = d_A.shape\n",
        "        P = d_B.shape[1]\n",
        "        sum = 0.0\n",
        "\n",
        "        for i in range(N):\n",
        "            sum += d_A[row, i] * d_B[i, col]\n",
        "\n",
        "        d_C[row, col] = sum\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    M, N, P = 3, 4, 2\n",
        "\n",
        "\n",
        "    h_A = np.random.randint(1, 10, size=(M, N)).astype(np.float32)\n",
        "    h_B = np.random.randint(1, 10, size=(N, P)).astype(np.float32)\n",
        "    h_C = np.empty((M, P), dtype=np.float32)\n",
        "\n",
        "    # Allocate device memory\n",
        "    d_A = cuda.to_device(h_A)\n",
        "    d_B = cuda.to_device(h_B)\n",
        "    d_C = cuda.device_array_like(h_C)\n",
        "\n",
        "    block_dim = (16, 16)\n",
        "    grid_dim = ((P + block_dim[1] - 1) // block_dim[1], (M + block_dim[0] - 1) // block_dim[0])\n",
        "\n",
        "    matMul[grid_dim, block_dim](d_A, d_B, d_C)\n",
        "\n",
        "    # Copy result to host\n",
        "    d_C.copy_to_host(h_C)\n",
        "\n",
        "\n",
        "    print(\"Matrix A:\")\n",
        "    print(h_A)\n",
        "    print(\"--------\")\n",
        "    print(\"Matrix B:\")\n",
        "    print(h_B)\n",
        "    print(\"--------\")\n",
        "    print(\"Matrix C (Result of Multiplication):\")\n",
        "    print(h_C)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoqYGeVhzGWY",
        "outputId": "dd47e408-13dc-44c5-ad30-b0d8c40b3a1d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "[[5. 9. 1. 5.]\n",
            " [6. 5. 2. 8.]\n",
            " [3. 1. 9. 5.]]\n",
            "--------\n",
            "Matrix B:\n",
            "[[7. 4.]\n",
            " [2. 9.]\n",
            " [5. 1.]\n",
            " [7. 5.]]\n",
            "--------\n",
            "Matrix C (Result of Multiplication):\n",
            "[[ 93. 127.]\n",
            " [118. 111.]\n",
            " [103.  55.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "\n",
        "@cuda.jit\n",
        "def matScale(d_A, d_B, scale):\n",
        "    row, col = cuda.grid(2)\n",
        "\n",
        "    if row < d_B.shape[0] and col < d_B.shape[1]:\n",
        "        d_B[row, col] = d_A[row, col] / scale\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    M, N = 4, 4\n",
        "\n",
        "\n",
        "    scale = 2.5\n",
        "\n",
        "\n",
        "    h_A = np.random.randint(1, 10, size=(M, N)).astype(np.float32)\n",
        "    h_B = np.empty_like(h_A)\n",
        "\n",
        "    # Allocate device memory\n",
        "    d_A = cuda.to_device(h_A)\n",
        "    d_B = cuda.to_device(h_B)\n",
        "\n",
        "    # Define block and grid dimensions\n",
        "    block_dim = (16, 16)\n",
        "    grid_dim = ((N + block_dim[1] - 1) // block_dim[1], (M + block_dim[0] - 1) // block_dim[0])\n",
        "\n",
        "    matScale[grid_dim, block_dim](d_A, d_B, scale)\n",
        "\n",
        "    # Copy result to host\n",
        "    d_B.copy_to_host(h_B)\n",
        "\n",
        "    # Display matrices and result\n",
        "    print(\"Matrix A:\")\n",
        "    print(h_A)\n",
        "    print(\"--------\")\n",
        "    print(\"Matrix B (Result of Scaling):\")\n",
        "    print(h_B)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4623srQHzXtH",
        "outputId": "f0ae5643-4d78-4f39-c2e4-7409f62c9efa"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "[[9. 5. 4. 5.]\n",
            " [8. 1. 3. 9.]\n",
            " [8. 5. 4. 9.]\n",
            " [1. 2. 7. 5.]]\n",
            "--------\n",
            "Matrix B (Result of Scaling):\n",
            "[[3.6 2.  1.6 2. ]\n",
            " [3.2 0.4 1.2 3.6]\n",
            " [3.2 2.  1.6 3.6]\n",
            " [0.4 0.8 2.8 2. ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "\n",
        "@cuda.jit\n",
        "def relu(d_a, d_b, alpha):\n",
        "    col = cuda.grid(1)\n",
        "\n",
        "    if col < d_b.shape[0]:\n",
        "        d_b[col] = max(alpha * d_a[col], d_a[col])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Vector size\n",
        "    N = 8\n",
        "\n",
        "    # Leaky ReLU parameter\n",
        "    alpha = 0.01\n",
        "\n",
        "\n",
        "    h_a = np.random.randint(-5, 8, size=N).astype(np.float32)\n",
        "    h_b = np.empty_like(h_a)\n",
        "\n",
        "    # Allocate device memory\n",
        "    d_a = cuda.to_device(h_a)\n",
        "    d_b = cuda.to_device(h_b)\n",
        "\n",
        "    block_dim = 256\n",
        "    grid_dim = (N + block_dim - 1) // block_dim\n",
        "\n",
        "    relu[grid_dim, block_dim](d_a, d_b, alpha)\n",
        "\n",
        "    # Copy result to host\n",
        "    d_b.copy_to_host(h_b)\n",
        "\n",
        "    print(\"Vector A:\")\n",
        "    print(h_a)\n",
        "    print(\"\\nLeaky ReLU:\")\n",
        "    print(h_b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K84MMRjYzlzp",
        "outputId": "8da4b3bd-e46b-4d60-e69e-a55f107bdc8d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector A:\n",
            "[-2.  6.  6.  2.  3. -5.  5.  5.]\n",
            "\n",
            "Leaky ReLU:\n",
            "[-0.02  6.    6.    2.    3.   -0.05  5.    5.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "\n",
        "@cuda.jit\n",
        "def softmax(d_in, d_out):\n",
        "    col = cuda.grid(1)\n",
        "\n",
        "    if col < d_out.shape[0]:\n",
        "        N = d_out.shape[0]\n",
        "\n",
        "        local_exp = math.exp(d_in[col])\n",
        "\n",
        "        # Allocate shared memory for parallel reduction\n",
        "        shared_exp = cuda.shared.array(256, dtype=float32)\n",
        "\n",
        "        # Store exp(x) in shared memory\n",
        "        shared_exp[cuda.threadIdx.x] = local_exp\n",
        "\n",
        "        # Synchronize!\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        # Parallel reduction to compute sum(exp(x))\n",
        "        for stride in range(cuda.blockDim.x // 2, 0, -1):\n",
        "            if cuda.threadIdx.x < stride:\n",
        "                shared_exp[cuda.threadIdx.x] += shared_exp[cuda.threadIdx.x + stride]\n",
        "\n",
        "            # Synchronize threads after each step of reduction\n",
        "            cuda.syncthreads()\n",
        "\n",
        "        # Calculate softmax for the current element\n",
        "        d_out[col] = local_exp / shared_exp[0]\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    # Vector size\n",
        "    N = 8\n",
        "\n",
        "    # Generate random input vector\n",
        "    h_in = np.random.randint(1, 8, size=N).astype(np.float32)\n",
        "    h_out = np.empty_like(h_in)\n",
        "\n",
        "    # Allocate device memory\n",
        "    d_in = cuda.to_device(h_in)\n",
        "    d_out = cuda.to_device(h_out)\n",
        "\n",
        "    block_dim = 256\n",
        "    grid_dim = (N + block_dim - 1) // block_dim\n",
        "\n",
        "    softmax[grid_dim, block_dim](d_in, d_out)\n",
        "\n",
        "    # Copy result to host\n",
        "    d_out.copy_to_host(h_out)\n",
        "\n",
        "    print(\"Softmax input:\")\n",
        "    print(h_in)\n",
        "    print(\"Softmax output:\")\n",
        "    print(h_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-49_2X-z93R",
        "outputId": "be15bf8d-832d-412e-ff2f-18f2ac27d9a7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax input:\n",
            "[2. 1. 6. 5. 7. 5. 4. 6.]\n",
            "Softmax output:\n",
            "[0.00111113 0.00040876 0.06066555 0.02231761 0.16490605 0.02231761\n",
            " 0.00821019 0.06066555]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def transpose_kernel(d_A, d_T, M, N):\n",
        "    row, col = cuda.grid(2)\n",
        "\n",
        "    if row < M and col < N:\n",
        "        d_T[col, row] = d_A[row, col]\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "  M = 4\n",
        "  N = 4\n",
        "\n",
        "  # Initialize host data\n",
        "  h_A = np.random.randint(1, 11, size=(M, N)).astype(np.float32)\n",
        "  h_T = np.empty((N, M), dtype=np.float32)\n",
        "\n",
        "  # Allocate device memory\n",
        "  d_A = cuda.to_device(h_A)\n",
        "  d_T = cuda.to_device(h_T)\n",
        "\n",
        "  block_dim = (16, 16)\n",
        "  grid_dim = ((N + block_dim[0] - 1) // block_dim[0], (M + block_dim[1] - 1) // block_dim[1])\n",
        "\n",
        "  transpose_kernel[grid_dim, block_dim](d_A, d_T, M, N)\n",
        "\n",
        "  # Copy result back to host\n",
        "  d_T.copy_to_host(h_T)\n",
        "\n",
        "  print(\"Matrix A:\")\n",
        "  print(h_A)\n",
        "  print(\"Transpose:\")\n",
        "  print(h_T)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0kLdjVj0LjP",
        "outputId": "b389700c-8d7a-4898-e8fd-1abf808e82a0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "[[ 1.  5.  2.  5.]\n",
            " [ 6.  8.  3.  6.]\n",
            " [10.  3.  2.  2.]\n",
            " [ 4.  4.  2.  5.]]\n",
            "Transpose:\n",
            "[[ 1.  6. 10.  4.]\n",
            " [ 5.  8.  3.  4.]\n",
            " [ 2.  3.  2.  2.]\n",
            " [ 5.  6.  2.  5.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "\n",
        "@cuda.jit\n",
        "def addVectors(d_a, d_b, d_c):\n",
        "    col = cuda.grid(1)\n",
        "\n",
        "    if col < d_c.shape[0]:\n",
        "        d_c[col] = d_a[col] + d_b[col]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    N = 10\n",
        "\n",
        "    h_a = np.arange(N, dtype=np.float32) - 3\n",
        "    h_b = np.arange(N, dtype=np.float32)\n",
        "    h_c = np.empty(N, dtype=np.float32)\n",
        "\n",
        "    # Allocate device memory\n",
        "    d_a = cuda.to_device(h_a)\n",
        "    d_b = cuda.to_device(h_b)\n",
        "    d_c = cuda.device_array_like(h_c)\n",
        "\n",
        "    block_dim = 256\n",
        "    grid_dim = (N + block_dim - 1) // block_dim\n",
        "\n",
        "    addVectors[grid_dim, block_dim](d_a, d_b, d_c)\n",
        "\n",
        "    # Copy result to host\n",
        "    d_c.copy_to_host(h_c)\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f'a: {h_a[i]} b: {h_b[i]} c: {h_c[i]}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktOfxAJE0wpv",
        "outputId": "0f7b2f68-4b55-4542-8c9c-19efb9bb861c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: -3.0 b: 0.0 c: -3.0\n",
            "a: -2.0 b: 1.0 c: -1.0\n",
            "a: -1.0 b: 2.0 c: 1.0\n",
            "a: 0.0 b: 3.0 c: 3.0\n",
            "a: 1.0 b: 4.0 c: 5.0\n",
            "a: 2.0 b: 5.0 c: 7.0\n",
            "a: 3.0 b: 6.0 c: 9.0\n",
            "a: 4.0 b: 7.0 c: 11.0\n",
            "a: 5.0 b: 8.0 c: 13.0\n",
            "a: 6.0 b: 9.0 c: 15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def  batch_matrix_multiplication(d_A, d_B, d_C, batch_size, M, N, P):\n",
        "    row, col, batch = cuda.grid(3)\n",
        "\n",
        "    if batch < batch_size and row < M and col < P:\n",
        "        sum = 0.0\n",
        "\n",
        "        for i in range(N):\n",
        "            sum += d_A[batch, row, i] * d_B[batch, i, col]\n",
        "\n",
        "        d_C[batch, row, col] = sum\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    M = 2\n",
        "    N = 3\n",
        "    P = 5\n",
        "    batch_size = 4\n",
        "\n",
        "    h_A = np.random.randint(1, 11, size=(batch_size, M, N)).astype(np.float32)\n",
        "    h_B = np.random.randint(1, 11, size=(batch_size, N, P)).astype(np.float32)\n",
        "    h_C = np.empty((batch_size, M, P), dtype=np.float32)\n",
        "\n",
        "    # Allocate device memory\n",
        "    d_A = cuda.to_device(h_A)\n",
        "    d_B = cuda.to_device(h_B)\n",
        "    d_C = cuda.device_array_like(h_C)\n",
        "\n",
        "    # Define block and grid dimensions\n",
        "    block_dim = (8, 8, batch_size)\n",
        "    grid_dim = ((P + block_dim[0] - 1) // block_dim[0], (M + block_dim[1] - 1) // block_dim[1], batch_size)\n",
        "\n",
        "    batch_matrix_multiplication[grid_dim, block_dim](d_A, d_B, d_C, batch_size, M, N, P)\n",
        "\n",
        "    # Copy result to host\n",
        "    d_C.copy_to_host(h_C)\n",
        "\n",
        "    for batch in range(batch_size):\n",
        "        print(f\"Matrix A (Batch {batch + 1}):\\n\", h_A[batch])\n",
        "        print(f\"Matrix B (Batch {batch + 1}):\\n\", h_B[batch])\n",
        "        print(f\"Matrix C (Batch {batch + 1}):\\n\", h_C[batch])\n",
        "        print(\"--------\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aMhiEre1JRc",
        "outputId": "21b6b357-13ee-43d2-e166-aae957b7a29a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A (Batch 1):\n",
            " [[ 4.  1.  1.]\n",
            " [ 4.  6. 10.]]\n",
            "Matrix B (Batch 1):\n",
            " [[ 6. 10.  5.  4.  3.]\n",
            " [ 8.  2.  1.  2.  7.]\n",
            " [ 8.  9. 10. 10.  8.]]\n",
            "Matrix C (Batch 1):\n",
            " [[ 40.  51.  31.  28.  27.]\n",
            " [152. 142. 126. 128. 134.]]\n",
            "--------\n",
            "Matrix A (Batch 2):\n",
            " [[7. 6. 1.]\n",
            " [9. 7. 8.]]\n",
            "Matrix B (Batch 2):\n",
            " [[ 6.  3.  4. 10.  8.]\n",
            " [ 3.  7.  5. 10.  9.]\n",
            " [ 7.  5.  2.  3.  4.]]\n",
            "Matrix C (Batch 2):\n",
            " [[ 67.  68.  60. 133. 114.]\n",
            " [131. 116.  87. 184. 167.]]\n",
            "--------\n",
            "Matrix A (Batch 3):\n",
            " [[4. 4. 8.]\n",
            " [7. 7. 2.]]\n",
            "Matrix B (Batch 3):\n",
            " [[ 8.  5.  6.  8. 10.]\n",
            " [ 9.  1.  6.  4.  8.]\n",
            " [ 5.  1.  8.  3.  9.]]\n",
            "Matrix C (Batch 3):\n",
            " [[108.  32. 112.  72. 144.]\n",
            " [129.  44. 100.  90. 144.]]\n",
            "--------\n",
            "Matrix A (Batch 4):\n",
            " [[2. 5. 4.]\n",
            " [1. 7. 6.]]\n",
            "Matrix B (Batch 4):\n",
            " [[ 1.  8.  4.  2.  7.]\n",
            " [ 3.  5.  3.  5.  6.]\n",
            " [ 8.  8. 10.  2.  2.]]\n",
            "Matrix C (Batch 4):\n",
            " [[49. 73. 63. 37. 52.]\n",
            " [70. 91. 85. 49. 61.]]\n",
            "--------\n"
          ]
        }
      ]
    }
  ]
}